# AUTOGENERATED! DO NOT EDIT! File to edit: 00_Fitting.ipynb (unless otherwise specified).

# Define the classes and functions that are loaded when importing *
__all__ = ['Metric', 'Functions', 'FittingFunction', 'BaseFit', 'Fit', 'DfFit', 'MultiFit', 'GroupFit',
           'OptimizeCriteria']

import numpy as np

import plotly
import plotly.express as px
import plotly.graph_objects as go

import pandas as pd

from .multiprocessing import PoolManager
from .fitting_core import Metric, Functions, FittingFunction, BaseFit, PlotFit, OptimizeCriteria

from WaterClassification import common
from scipy.optimize import curve_fit, minimize

from itertools import product
from datetime import datetime

import warnings
warnings.filterwarnings('ignore')

# To solve convergence problems?
# https://stackoverflow.com/questions/56164659/scipy-curve-fit-do-not-converge-even-if-i-iteratively-change-initial-guess


class Fit:
    """
    Base class of the fitting module. It stores ONE fit with original data and fitting parameters.

    There are some available functions ['linear', 'expo', 'power', 'nechad'] but other functions can be
    passed as parameter to the fitting

    Different metrics can be used to minimize the error ['r2', 'rmse', 'rmsle', 'SSE']
    Original data and fitted parameters keeps stored in the class.
    """
    def __init__(self, func, x, y, metrics=None, optimize_metric=None):
        """
        Fits a model (function) to a set of data (x, y), minimizing the optimize metric.
        Stores the parameters and the errors as a dictionary.
        :param func: math model y = f(x). Signature should be func_name(x, *params) -> y
        It can also be a string from the list: ['linear', 'expo', 'power', 'nechad']
        :param x: independent variable
        :param y: targets or dependent variable
        :param metrics: The metrics to be calculated after fitting. If none, use BaseFit.available_metrics
        :param optimize_metric: If informed, the algo will use the optimize metric's loss function to minimize
                                its errors. That's an extra step.
        """

        self.metrics = BaseFit.available_metrics if metrics is None else metrics
        self.func = Functions.fitting_func(func)

        # save initial parameters
        self.fit_params = {'func': self.func,
                           'qty': len(x),
                           'optimize_metric': optimize_metric,
                           'x': x,
                           'y': y}

        # initialize a log list to keep track of the logging
        self.log = []
        try:
            # make a non-linear least squares fit
            popt, pcov = curve_fit(self.func.func, x, y, check_finite=False, )

            # if there is a metric to be optimized, use the previous params to re-fit
            if optimize_metric is not None:
                res = minimize(optimize_metric.loss_func,  x0=popt,
                               args=(self.func.func, x, y), method='Nelder-Mead', tol=0.001)
                popt = res.x

            # update the fit_params with the results
            self.fit_params.update(BaseFit.test_fit(x, y, self.func, popt, metrics=self.metrics))

        except Exception as err:
            self.log.append(err)
            pass

    def html_metrics(self):
        """Return the fitted metrics formatted as HTML"""
        html_metrics = ''
        for metric in self.metrics:
            html_metrics += f'{metric}: {self.fit_params[metric]}<br>'
        return html_metrics

    def _scatter_plot(self, **kwargs):
        return px.scatter(x=self.fit_params['x'], y=self.fit_params['y'], **kwargs)

    def plot_fit(self, new_funcs=None, new_params=None, pts=100, fig=None, position=(1, 1), update_layout=None,
                 **kwargs):
        """
        Plots the fit (data and curve). If existing fig is passed, the position parameter indicates
        where (row, col tuple) it will be placed in a given figure.

        :param new_funcs: New function(s) to be added to the plot (tom compare the traces)
        :param new_params: The parameter(s) for the new function(s)
        :param pts: Number of points to be used in the function trace
        :param fig: Existing figure to collate this plot
        :param position: (row, column) of the fig to collate this new plot
        :param update_layout: At the end, update_layout function is called on the resulting figure to final adjustments
                              This parameter is a dictionary of update layout arguments ex. {'showlegend': False}
        :param kwargs: Other Plotly Express arguments that will be used when creating the Scatter of the points
        :return: Figure object
        """

        # create a scatter with the points
        scatter_plot = self._scatter_plot(**kwargs)

        fig = scatter_plot if fig is None else common.apply_subplot(fig, scatter_plot, position)

        # Create the original function trace if the function is fitted, otherwise, skip this trace
        if not self.is_empty:
            traces = [PlotFit.draw_func_trace(func=self.func,
                                              params=self.fit_params['params'],
                                              x_interval=self.fit_params['x'],
                                              txt=self.html_metrics())]
        else:
            traces = []

        # Append the traces for the "passed" functions (for comparison purposes)
        new_funcs, new_params = common.listify(new_funcs, new_params)
        for new_func, new_param in zip(new_funcs, new_params):
            traces.append(PlotFit.draw_func_trace(new_func, new_param, x_interval=self.fit_params['x']))

        # Add all function traces to the figure
        for trace in traces:
            fig.add_trace(trace, row=position[0], col=position[1])

        # After the traces are all drawn, call update layout to fine-tune
        fig.update_layout(update_layout)

        return fig

    def summary(self, params=None):
        """Returns just a subset of the fit_params"""
        params = BaseFit.summary_params if params is None else common.listify(params)

        res = {}
        for param in params:
            res[param] = self.get_param(param)

        return res

    def get_param(self, param_name, default_value=None):
        return self.fit_params.get(param_name, default_value)

    @property
    def is_empty(self):
        return 'params' not in self.fit_params.keys()

    def __repr__(self):
        s = f"{repr(self.func)} - qty: {self.fit_params['qty']} | "

        if self.is_empty:
            return s + 'Empty fitting'
        else:
            for key in self.metrics:
                s += f"{key}: {self.get_param(key)} "
            return s


class DfFit(Fit):
    """
    Derives from the Fit class and adds the support to work with DataFrames.
    """
    def __init__(self, df, func, expr_x, expr_y, metrics=None, optimize_metric=None):
        """Fits a function using the dataframe and expressions for x and for Y."""
        x = BaseFit.parse_expr_df(df, expr_x)
        y = BaseFit.parse_expr_df(df, expr_y)

        super().__init__(func, x=x, y=y, metrics=metrics, optimize_metric=optimize_metric)

        self.df = df
        self.expr_x = expr_x
        self.expr_y = expr_y

        self.fit_params.update({'idx': df.index.to_list(), 'band': expr_x, 'variable': expr_y})

    # ################################  PLOTTING METHODS  #################################
    def _scatter_plot(self, **kwargs):
        """Overrides the original scatter_plot from Fit, to pass the dataframe"""
        return px.scatter(self.df, x=self.fit_params['x'], y=self.fit_params['y'], **kwargs)

    def plot_fit(self, new_funcs=None, new_params=None, fig=None, position=(1, 1), update_layout=None,
                 **kwargs):

        if 'hover_data' in kwargs:
            kwargs['hover_data'] = [self.df.index] + common.listify(kwargs['hover_data'])
        else:
            kwargs['hover_data'] = [self.df.index]

        # Use the plot from upper class Fit
        fig = super().plot_fit(position=position, new_funcs=new_funcs, new_params=new_params, fig=fig,
                               title=self.title, update_layout=update_layout, **kwargs)

        fig.update_xaxes(title_text=self.x_label, row=position[0], col=position[1])
        fig.update_yaxes(title_text=self.y_label, row=position[0], col=position[1])

        return fig

    def plot_pred_vs_targ(self, fig=None, position=None, update_layout=None, **kwargs):

        # Add the index to the hover_data argument
        if 'hover_data' in kwargs:
            kwargs['hover_data'] = [self.df.index] + common.listify(kwargs['hover_data'])
        else:
            kwargs['hover_data'] = [self.df.index]

        max_value = max(self.df[self.expr_y].max(), max(self.fit_params['y_hat']))
        min_value = min(self.df[self.expr_y].min(), min(self.fit_params['y_hat']))

        plot = px.scatter(self.df, x=self.expr_y, y=self.fit_params['y_hat'], **kwargs)
        plot.add_trace(go.Scatter(x=[min_value, max_value], y=[min_value, max_value], mode='lines'))

        fig = plot if fig is None else common.apply_subplot(fig, plot, position)

        fig.update_layout(update_layout)
        return fig

    # ################################  OTHER METHODS  #################################
    @property
    def title(self): return f'{repr(self.func)} on {self.expr_x}'

    @property
    def x_label(self):
        return f'Band {self.expr_x} nm' if self.expr_x in self.df.columns else f'Expression {self.expr_x}'

    @property
    def y_label(self): return self.expr_y

    def __repr__(self):
        return f'Band {self.expr_x} ' + super().__repr__()

    def __len__(self):
        return len(self.df)


class MultiFit:
    """
    Uses parallel processing to fit several functions and several independent variables (lst_expr_x) to the
    dependent variable (expr_y) and return a dataframe with the results. Each fit is stored in the .fits list
    """
    def __init__(self, df, lst_expr_x, funcs, expr_y='SPM', metrics=None, metric=BaseFit.rmsle, n_cpus=None, pool=None,
                 optimize_metric=False):
        """
        Performs several DfFits and save the results to get best results.
        :param df: Dataframe with data to be fitted
        :param lst_expr_x: columns/expressions to be tested
        :param funcs: list of functions to be tested
        :param expr_y: independent variable
        :param metric: the metric to optimized
        :param n_cpus: Number of CPUs to use in the processing. If None, no parallel processing
        :param pool: existing pool to process the results. If None, a new pool is created
        :param optimize_metric: If True, go thorough a second step minimizing the loss function of the metric
        """

        self.start = datetime.now()
        self.df = df

        # If there was not multiprocessing, the solution would be a couple of for's loops to call DfFit
        # self.fits = []
        # for expr_x in lst_expr_x:
        #     for func in funcs:
        #         self.fits.append(DfFit(self.df, func, expr_x, expr_y, metric=metric))

        # However, to keep the code cleaner multi and single core will use MAPPING functions
        # First we have to create the arguments by combining the funcs and expressions
        # Prepare data to be sent to pool into an array by doing a cross product on the lists

        # if optimize metric is True, send the metric to be optimized as a parameter to the fit engine
        optimize_metric = metric if optimize_metric else None

        # data = np.array(list(product([df], funcs, lst_expr_x, [expr_y], [metrics], [optimize_metric])))
        data = np.array(list(product(funcs, lst_expr_x)))

        # In this part, we will exclude the combinations of that expect 1 independent variable and receives 2
        dims = np.fromiter(map(lambda x: x.dim, data[:, 0]), 'uint8')
        tuples = np.fromiter(map(lambda x: isinstance(x, tuple), data[:, 1]), 'bool')

        to_exclude = (dims == 1) & tuples

        self.data = np.delete(data, np.where(to_exclude), axis=0)

        print(f"Data is ready! {len(self.data)} records!")

        with PoolManager(n_cpus, pool) as pool_mgr:
            # self.fits = pool_mgr.map(DfFit, data[:, 0], data[:, 1], data[:, 2], data[:, 3], data[:, 4], data[:, 5])
            self.fits = pool_mgr.map(DfFit,
                                     [df]*len(self.data),
                                     self.data[:, 0],
                                     self.data[:, 1],
                                     [expr_y]*len(self.data),
                                     [metrics]*len(self.data),
                                     [optimize_metric]*len(self.data))

        # Save the parameters
        self.lst_expr_x = lst_expr_x
        self.lst_funcs = funcs
        self.expr_y = expr_y
        self.metric = metric

        # At the end, we will keep the fits sorted by metric
        self.sort()
        self.end = datetime.now()

    @staticmethod
    def fit_filtered(df, lst_expr_x, funcs, filter_column, filter_value, thresh=10, expr_y='SPM', metric=BaseFit.rmsle,
                     n_cpus=None, pool=None, metrics=None, optimize_metric=None):
        """Filters the dataframe by column value and gets the best fit"""

        sub_df = df[df[filter_column] == filter_value]

        if len(sub_df) > thresh:
            return MultiFit(sub_df, lst_expr_x, funcs, expr_y, metric=metric, n_cpus=n_cpus, pool=pool,
                            metrics=metrics, optimize_metric=optimize_metric)
        else:
            return None

    def sort(self, metric=None, reverse=False):
        """
        Sort the fits to put them from the best to the worst according to the metric (unless reverse is True)
        :param metric: metric for sorting
        :param reverse: if False (default), will put best fits first
        :return: Updates internal list of fits
        """
        # Update the metric or leave as is
        self.metric = metric if metric is not None else self.metric

        # Call the base function for sorting fits
        self.fits = BaseFit.sort_fits(self.fits, self.metric, reverse)

    def summary(self, params=None, n=5):

        params = BaseFit.summary_params if params is None else params

        # create a dictionary with groups and respective params. If group is none, fill it with NaNs
        results = {fit.title: fit.summary(params=params) if fit is not None else {param: None for param in params}
                   for fit in self.fits[:n]}

        return pd.DataFrame(results).T

    @property
    def elapsed_time(self):
        return f'{(self.end-self.start).total_seconds():.2f} seconds'

    # ################################  PLOTTING METHODS  #################################
    def plot_best_fits(self, n=6, cols=2, base_height=300, **kwargs):
        """
        Plot the fits in the MultiFit instance. Considering that the MultiFit can store enormous quantity of
        fits, this function will be limited to the "best" n fits, considering the fits are ordered by the
        given metric. If one wants to order differently, it is possible to sort the MultiFit.fits using
        BaseFit.sort_fits function and passing the corresponding metric.
        :param n: number of fits to plot
        :param cols: number of columns to divide the canvas
        :param base_height: high of each row
        :param kwargs: additional arguments to be used in the base px.scatter call
        :return: plotly figure
        """
        # First, we will check the max n in the range to avoid throwing exceptions
        n = min(n, len(self.fits))
        fits = self.fits[:n]

        return PlotFit.plot_fits(fits, cols, base_height, **kwargs)

    def plot_fits(self, plots, cols=1, base_height=300, **kwargs):

        # assure plot is a list of tuples
        plots = common.listify(plots)

        # get the desired fits
        fits = [self.get_fit(*plot) for plot in plots]

        return PlotFit.plot_fits(fits, cols, base_height, **kwargs)

    def plot_best_fit(self, **kwargs):
        return self.best_fit.plot_fit(**kwargs)

    def plot_metric(self, metric=None, max_n=50, fig=None, position=None):

        # Create the metric figure
        metric = self.metric if metric is None else metric

        x = [fit.title for fit in self.fits[:max_n]]
        y = [fit.get_param(metric) for fit in self.fits[:max_n]]

        df = pd.DataFrame({'Function': x, str(metric): y})
        metric_fig = px.line(df, x='Function', y=str(metric))

        # If there was an existing figure, just apply the new figure
        fig = metric_fig if fig is None else common.apply_subplot(fig, metric_fig, position)

        return fig

    # ################################  MANAGE FITS METHODS  #################################
    def get_fit(self, func, band):
        for fit in self.fits:
            if (fit.fit_params['func'] == func) and (fit.fit_params['band'] == band):
                return fit

    @property
    def best_fit(self): return self.fits[0]

    def __repr__(self):
        s = f'Multifit class with {len(self.fits)} fits on variable ({self.expr_y}): df with {len(self.df)} rows, bands:{self.lst_expr_x} funcs{self.lst_funcs} '
        return s

    def __len__(self):
        return len(self.df)

    #     def plot_multifit(self, rows, cols, plots, positions, base_height=300, **kwargs):
    #
    #         titles = [self.get_fit(*plot).title for plot in plots]
    # #         forlist(map(lambda x: f'Function {x[0]} on band {x[1]}', plots))
    #
    #         fig = plotly.subplots.make_subplots(rows=rows, cols=cols, subplot_titles=titles)
    #
    #         for plot, position in zip(plots, positions):
    #             fit = self.get_fit(*plot)
    #             fig = fit.plot_fit(fig=fig, position=position, **kwargs)
    #
    #         fig.update_layout(height=base_height*rows)
    #
    #         return fig.update_coloraxes(colorscale='Plasma')

    # def get_best_fit(self, metric='rmsle', criteria='min'):
    #     "Retrieve the summary of the best fit among the fits within this object"
    #     fits = self.get_results_df()
    #
    #     scores = fits.loc[(fits.index.levels[0], metric), :]
    #     idx_name, band = MultiFit.get_best_score(scores, criteria=criteria)
    #
    #     res_df = fits.loc[idx_name[0][0], band[0]]
    #     return res_df.append(pd.Series({'band': band[0], 'func': idx_name[0][0], 'qty': len(self.df), 'ids': self.df['Id'].to_list()}))
    #
    # def get_best_fit_obj(self, metric='rmsle', criteria='min'):
    #     "Retrieve the actual best fit object. Uses get_best_fit to decide among the results."
    #     best_fit = self.get_best_fit(metric=metric, criteria=criteria)
    #
    #     return self.get_fit(best_fit['func'], best_fit['band'])
    #
    # @staticmethod
    # def get_best_score(df, criteria='max', priority_funcs=['linear', 'power', 'nechad', 'expo']):
    #     "Given a dataframe with scores, get the best combination (function and band), that meets the logic criteria (max or min)"
    #
    #     # Get the function for the criteria
    #     criteria = pd.DataFrame.max if criteria=='max' else pd.DataFrame.min
    #
    #     # Get the lists of indexes and columns of elements that meet the criteria
    #     idxs, cols = np.where(df == criteria(criteria(df, skipna=True)))
    #
    #     # If there is more than one result, we have to get the one in the higher function piority
    #     idxs_names = df.index[idxs]
    #
    #     # Get the loc of the best solution given the priority functions
    #     loc = 0
    #     for func in priority_funcs:
    #         try:
    #             loc = idxs_names.get_loc(func)
    #
    #             # If the function appears more than once, gets the first result
    #             # TODO: update to work with priority bands combinations
    #             if isinstance(loc, np.ndarray): loc = np.where(loc == True)[0][0]
    #
    #             break
    #
    #         except:
    #             pass
    #
    #     idxs = [idxs[loc]] if isinstance(loc, np.int64) else idxs[loc]
    #     cols = [cols[loc]] if isinstance(loc, np.int64) else cols[loc]
    #     return df.index[idxs], df.columns[cols]
    #
    # #################################  OTHER METHODS  #################################
    # def get_title(self, metric='rmsle', criteria='min'):
    #     best_fit = self.get_best_fit_obj(metric=metric, criteria=criteria)
    #     return f'func {best_fit.func.__name__} on {best_fit.expr_x}'
    #
    # def get_results_df(self):
    #     results = {}
    #
    #     for expr_x in self.lst_expr_x:
    #         res_func = {}
    #         for func in self.lst_funcs:
    #             # get the saved fit
    #             fit = self.get_fit(func, expr_x)
    #
    #             # func must be in string format
    #             func = func if isinstance(func, str) else func.__name__
    #
    #             for key in self.results_vars:
    #                 res_func.update({(func, key): fit.fit_params[key] if fit and fit.filled() else None})
    #
    #         results.update({expr_x: res_func})
    #
    #     return pd.DataFrame.from_dict(results)
    #
    # def get_results_transposed(self, results_vars=None):
    #     results_vars = self.results_vars if results_vars is None else results_vars
    #
    #     results = {}
    #     for fit in self.fits:
    #         if fit.filled():
    #             values = {var: fit.fit_params[var] for var in results_vars}
    #             results.update({(fit.fit_params['func'], fit.fit_params['band']): values})
    #
    #     return pd.DataFrame(results).T


class GroupFit:
    def __init__(self, df, bands, funcs, group_column, expr_y='SPM', metric=BaseFit.rmsle, verbose=False,
                 ignore_none=True, thresh=10, n_cpus=None, pool=None, metrics=None, optimize_metric=False, ):
        """Loop through the unique values of the group_column and for each group,
        perform MultiFits according to bands and functions"""

        self.group_fits = {}

        with PoolManager(n_cpus, pool) as pool_mgr:
            for group in sorted(df[group_column].unique()):
                if verbose:
                    print(f'Fitting group={group}')
                group_fit = MultiFit.fit_filtered(df, bands, funcs, group_column, group, thresh=thresh,
                                                  expr_y=expr_y, metric=metric, n_cpus=n_cpus, pool=pool_mgr.pool,
                                                  metrics=metrics, optimize_metric=optimize_metric)

                if not ignore_none or (group_fit is not None):
                    self.group_fits[group] = group_fit

        self.df = df
        self.bands = bands
        self.funcs = funcs
        self.group_column = group_column
        self.variable = expr_y
        # self.metric = metric
        # self.metrics = metrics

        self.overall = self.calc_overall_metric()

    def summary(self, params=None):

        params = BaseFit.summary_params if params is None else params

        # create a dictionary with groups and respective params. If group is none, fill it with NaNs
        results = {group: fit.best_fit.summary(params=params) if fit is not None else {param: None for param in params}
                   for group, fit in self.group_fits.items()}

        # Add the overall with the desired params (the overall has already been created with all available metrics)
        results['overall'] = {param: self.overall[param] for param in params if param in self.overall}

        return pd.DataFrame.from_dict(results, orient='index')

    def sort_fits(self, metric=None, reverse=False):
        """
        Sort the fits of each group_fit (MultiFit instances) based on a metric
        :param metric: metric to be used for ordering the fits
        :param reverse: If True, each group will be sorted from the worst to the best
        :return: Each group_fit will be sorted internally and overall metric will be updated
        """
        for multi_fit in self.group_fits.values():
            multi_fit.sort(metric, reverse)

        self.overall = self.calc_overall_metric()

    def sort_groups(self, reverse=False):
        """
        Sort the groups (keys) in the .group_fit dictionary.
        :return: None
        """
        self.group_fits = {group: self[group] for group in sorted(self.group_fits.keys(), reverse=reverse)}

    def rename_groups(self, mapping):
        """
        Rename the groups according to a dictionary mapping {from_value: to_value}
        :return: The groups will be renamed accordingly
        """
        self.group_fits = {mapping[group]: self[group] for group in mapping if group in self.group_fits.keys()}

    def save_memory(self, max_n=10):
        """
        Save memory by deleting non-relevant fits and leaving just the best fits.
        :param max_n: maximum number of fits to be stored
        :return: None
        """
        for mfit in self.group_fits.values():
            mfit.fits = mfit.fits[:max_n]

    def calc_overall_metric(self):
        # prepare targets and predictions lists. These values are already saved in each best_fit
        preds, targs, groups = [], [], []

        # loop through the group_fits to get the predictions
        pred_column = self.variable + '_pred'
        for group, multi_fit in self.group_fits.items():
            if multi_fit is not None:
                self.df.loc[multi_fit.best_fit.get_param('idx'), pred_column] = multi_fit.best_fit.get_param('y_hat')

        # once we have all these values, let's calculate the metrics
        results = BaseFit.test_fit(self.df[pred_column], self.df[self.variable], func=None,
                                   metrics=BaseFit.available_metrics)
        results['qty'] = len(preds)
        results.pop('y_hat', None)
        return results

    def group_title(self, group): return 'Group ' + str(group) + ' - ' + self[group].best_fit.title

    # ################################  PLOT METHODS  #################################
    def plot_group(self, group, **kwargs):
        return self.group_fits[group].plot_best_fit(**kwargs)

    def plot_groups(self, groups=None, cols=2, base_height=300, **kwargs):

        # get the groups to be plotted
        groups = self.group_fits if groups is None else common.listify(groups)

        # create a base configuration and override it with the kwargs
        base_options = {'update_layout': {'showlegend': False}}
        base_options.update(kwargs)

        # get the fits to be plotted
        fits = [self.group_fits[group].best_fit for group in groups]

        # get the titles
        titles = [self.group_title(group) for group in self.group_fits]

        return PlotFit.plot_fits(fits, cols, base_height, titles=titles, **base_options)

    def plot_groups_summary(self, groups=None, max_n=50, base_height=500, **kwargs):
        # get the groups to be plotted
        groups = self.group_fits if groups is None else common.listify(groups)

        # create a base configuration and override it with the kwargs
        base_options = {'update_layout': {'showlegend': False}}
        base_options.update(kwargs)

        # get the titles
        titles = [[self.group_title(group), 'Actual vs Estimated'] for group in groups]
        titles = ['Actual vs Estimated (all vallues)'] + list(np.array(titles).reshape(-1))

        # create the subplots
        specs = [[{"colspan": 2}, None]] + [[{}, {}]] * len(groups)
        fig = plotly.subplots.make_subplots(rows=len(groups)+1, cols=2, specs=specs, subplot_titles=titles)

        common.apply_subplot(fig, self.plot_pred_vs_targ(log_x=True, log_y=True), (1, 1))

        # loop through the fits to plot them in the main figure
        for row, group in enumerate(groups):
            # Plot the curve fit
            self[group].best_fit.plot_fit(fig=fig, position=(row+2, 1), **kwargs)

            # Plot the metric decay
            # self[group].plot_metric(max_n=max_n, fig=fig, position=(row+2, 1))
            self[group].best_fit.plot_pred_vs_targ(fig=fig, position=(row+2, 2), log_x=True, log_y=True, **kwargs)

        # update the final layout
        update_layout = {'height': base_height * len(groups),
                         'showlegend': False}

        # Override with 'new' options from kwargs
        if 'update_layout' in kwargs:
            update_layout.update(kwargs['update_layout'])

        fig.update_layout(update_layout)

        fig.update(layout_coloraxis_showscale=False)
        return fig

    def plot_pred_vs_targ(self, color=None, **kwargs):
        pred_column = self.variable + '_pred'
        # check if the variables are already in overall, update the metrics
        if pred_column not in self.df.columns:
            self.overall = self.calc_overall_metric()

        color = self.group_column if color is None else color

        # force a color mapping so the cluster 0 has the first color and so on...
        colors = px.colors.qualitative.Plotly + px.colors.qualitative.Light24
        color_discrete_map = {key: colors[i] for i, key in enumerate(self.group_fits.keys())}

        fig = px.scatter(self.df, x=self.variable, y=pred_column, color=color, color_discrete_map=color_discrete_map,
                         hover_data=[self.df.index, 'Area', 'Station'], **kwargs)

        # trace the y=x line
        max_value = self.df[[self.variable, pred_column]].max().max()
        min_value = self.df[[self.variable, pred_column]].min().min()

        fig.add_trace(go.Scatter(x=[min_value, max_value], y=[min_value, max_value], mode='lines'))

        return fig.update_layout(showlegend=False)

    def __repr__(self):
        s = f'Grouped fitting with {len(self.group_fits)} groups: {list(self.group_fits.keys())}'
        return s

    def __getitem__(self, item):
        return self.group_fits[item]

    # def plot_groups(self, rows, cols, groups=None, showlegend=True, base_height=300, **kwargs):
    #     "Plot several groups at once"
    #     groups = self.group_fits.keys() if groups is None else groups
    #
    #     titles = [f'{group}: ' + self.group_fits[group].get_title(metric=self.metric, criteria=self.criteria) for group in groups]
    #
    #     fig = plotly.subplots.make_subplots(rows=rows, cols=cols, subplot_titles=titles)
    #
    #     for i, group in enumerate(groups):
    #         group_plot = self.plot_group(group, **kwargs)
    #         fig = core.apply_subplot(fig, group_plot, self.plot_position(cols, i))
    #
    #     fig.update_layout(showlegend=showlegend, height=base_height*rows)
    #     fig.update_coloraxes(colorscale='Plasma')
    #     return fig

    # def get_grouped_results(self):
    #     "Adds the overall result (considering all points) to the results table."
    #     results = self.get_results()
    #     grouped_metric = self.calc_grouped_metric_()
    #     return results.append(pd.Series({key: grouped_metric[key] for key in ['r2', 'rmse', 'rmsle', 'SSE', 'qty', 'ids']}).rename('Overall'))
    #
    # # def calc_grouped_metric_(self):
    # #     "Calculates the metrics considering all the best fits in the group and return as a dictionary."
    # #     preds = []
    # #     targs = []
    # #     for group_fit in self.group_fits.values():
    # #         fit = group_fit.get_best_fit_obj(metric=self.metric, criteria=self.criteria)
    # #         preds = preds + list(fit.fit_params['y_hat'])
    # #         targs = targs + list(fit.fit_params['y'])
    # #
    # #     grouped_metric = Fit.test_fit(preds, targs, func=None)
    # #     grouped_metric.update({'ids': self.df.index.to_list()})
    # #     return grouped_metric
    #
    # def update_group_column(self, group_column='group'):
    #     "Updates the group column, considering the group_fits stored"
    #
    #     self.group_column = group_column
    #
    #     # sets the correct index
    #     self.df.set_index('Id', inplace=True)
    #
    #     for i, group_fit in enumerate(self.group_fits):
    #         sub_df = self.group_fits[group_fit].df.set_index('Id')
    #         self.df.loc[sub_df.index, group_column] = str(i)
    #
    #     self.df.reset_index(inplace=True)
    #     return self.df

    # def plot_scatter(self, x='area', y='SPM', update_group_column='group', log_y=True, title='', marker_size=3):
    #
    #     if update_group_column: self.update_group_column(update_group_column)
    #
    #     title += '<br>' if title != '' else ''
    #     title += f'Groups: {len(self.group_fits)} | {self.metric}={self.calc_grouped_metric_()[self.metric]:.2f}'
    #
    #     fig = px.scatter(self.df, x='area', y='SPM', color=self.df[self.group_column].astype('str'),
    #                      log_y=log_y, title=title)
    #
    #     fig.update_traces(marker=dict(size=marker_size))
    #     return fig

    # def plot_mean_reflectances(self, update_group_column=None, bands=None, title=''):
    #
    #     if update_group_column: self.update_group_column(update_group_column)
    #
    #     bands = core.s2bands if bands is None else bands
    #
    #     title += '<br>' if title != '' else ''
    #     title += f'Groups: {len(self.group_fits)} | {self.metric}={self.calc_grouped_metric_()[self.metric]:.2f}'
    #
    #     fig = core.plot_mean_reflectances(self.df, group_by=self.group_column,
    #                                       id_vars=[self.group_column, self.variable, 'area'],
    #                                       color=self.group_column, bands=bands,
    #                                       title=title
    #                                      )
    #     return fig

# Cell
# def sorted_k_partitions(seq, k):
#     """Returns a list of all unique k-partitions of `seq`.
#
#     Each partition is a list of parts, and each part is a tuple.
#
#     The parts in each individual partition will be sorted in shortlex
#     order (i.e., by length first, then lexicographically).
#
#     The overall list of partitions will then be sorted by the length
#     of their first part, the length of their second part, ...,
#     the length of their last part, and then lexicographically.
#     """
#     n = len(seq)
#     groups = []  # a list of lists, currently empty
#
#     def generate_partitions(i):
#         if i >= n:
#             yield list(map(tuple, groups))
#         else:
#             if n - i > k - len(groups):
#                 for group in groups:
#                     group.append(seq[i])
#                     yield from generate_partitions(i + 1)
#                     group.pop()
#
#             if len(groups) < k:
#                 groups.append([seq[i]])
#                 yield from generate_partitions(i + 1)
#                 groups.pop()
#
#     result = generate_partitions(0)
#
#     # Sort the parts in each partition in shortlex order
#     result = [sorted(ps, key = lambda p: (len(p), p)) for ps in result]
#     # Sort partitions by the length of each part, then lexicographically.
#     result = sorted(result, key = lambda ps: (*map(len, ps), ps))
#
#     return result
#
#
# def test_criteria(v1, v2, criteria):
#     if criteria == 'min':
#         return v1 < v2
#     else:
#         return v1 > v2
#
# class ConstrainedGFit(GroupFit):
#     def __init__(self, df, bands, funcs, group_column, k_partitions, variable='SPM', metric='rmsle', criteria='min', ignore_none=True, thresh=10):
#
#         best_value = np.inf if criteria == 'min' else -np.inf
#         self.partitions = {}
#
#         for partition in sorted_k_partitions(df[group_column].unique(), k_partitions):
#
#             # Create the Merged_Groups column
#             self.apply_partition_df(df, group_column, partition)
#
#             # Calc the fitting using parent's constructor
#             super().__init__(df, bands, funcs, 'Merged_Groups', variable, metric, criteria, ignore_none, thresh)
#
#             # Test the criteria
#             metric_value = self.calc_grouped_metric_()[metric]
#
#             self.partitions.update({str(partition): metric_value})
#
#             if test_criteria(metric_value, best_value, criteria):
#                 self.best_partition = partition
#                 best_value = metric_value
#
#         self.apply_partition_df(df, group_column, self.best_partition)
#         super().__init__(df, bands, funcs, 'Merged_Groups', variable, metric, criteria, ignore_none, thresh)
#
#     def apply_partition_df(self, df, group_column, partition):
#
#         # Reset the names of the merged groups to original groups
#         df['Merged_Groups'] = df[group_column]
#
#         # for each group inside the partition, correct the Merged_Groups names
#         for group in partition:
#             merged_group_name = '_'.join([str(g) for g in group])
#
#             # for each item in group, set it merged_group_name
#             for group_item in group: df.loc[df[group_column]==group_item, 'Merged_Groups'] = merged_group_name
#
#
#
#
# # Cell
# def dic_copy(dic_src, skip_keys=[]):
#     result_dic = {}
#     for key, item in dic_src.items():
#         if key not in skip_keys:
#             result_dic.update({key:item})
#     return result_dic
#
# def split_by_key(gfit, split_key, cluster_bands, cluster_column):
#     mfit = gfit.group_fits[split_key]
#
#     df = core.clusterize(mfit.df, cluster_bands, n_clusters=2, cluster_column=cluster_column)
#     new_gfit = GroupFit(df, gfit.bands, Fit.available_funcs, cluster_column, variable=gfit.variable,
#                         metric=gfit.metric, criteria=gfit.criteria)
#
#     # create a new dictionary of fits and copy to it the old fits (except the one splitted)
#     new_fits = dic_copy(gfit.group_fits, skip_keys=[split_key])
#
#     # insert into the new dictionary the splitted fits
#     for i, item in enumerate(new_gfit.group_fits.values()):  new_fits.update({f'{split_key}_{i}': item})
#
#     # change the dictionary of the original gfit
#     gfit.group_fits = new_fits
#
#     return gfit
#
#
#
# def split_GroupFit(base_gfit, cluster_bands, cluster_column):
#     "Uses the same variable, metric and criteria of original GroupFit"
#
#     gfit = GroupFit(base_gfit.df, base_gfit.bands, Fit.available_funcs, base_gfit.group_column,
#                     base_gfit.variable, base_gfit.metric, base_gfit.criteria, calc=False)
#
#     gfit.group_fits = base_gfit.group_fits.copy()
#
#     baseline = gfit.calc_grouped_metric_()[gfit.metric]
#     split_key = list(gfit.group_fits.keys())[0]
#
#     print(f'Actual {gfit.metric}={baseline}')
#
#     # backup the original gfit dictionary
#     group_fits_backup = gfit.group_fits.copy()
#
#     for key, mfit in group_fits_backup.items():
#
#         for bands in [cluster_bands, core.wavelength_range(375, 940, 10)]:
#             gfit = split_by_key(gfit, key, bands, cluster_column)
#
#             # test the metric with this new configuration
#             metric_value = gfit.calc_grouped_metric_()[gfit.metric]
#
#             if test_criteria(metric_value, baseline, gfit.criteria):
#                 baseline = metric_value
#                 split_key = key
#                 split_bands = bands
#
#             # restore the original fits
#             gfit.group_fits = group_fits_backup
#
#     print(f'Splitting {split_key}: {gfit.metric}={baseline}')
#
#     return split_by_key(gfit, split_key, split_bands, cluster_column)
