# AUTOGENERATED! DO NOT EDIT! File to edit: 00_Fitting.ipynb (unless otherwise specified).

# Define the classes and functions that are loaded when importing *
__all__ = ['Metric', 'Functions', 'FittingFunction', 'BaseFit', 'Fit', 'DfFit', 'MultiFit', 'GroupFit',
           'OptimizeCriteria']

import numpy as np

import plotly
import plotly.express as px
import plotly.graph_objects as go

import pandas as pd

from .multiprocessing import PoolManager
from .fitting_core import Metric, Functions, FittingFunction, BaseFit, PlotFit, OptimizeCriteria

from .. import common
from scipy.optimize import curve_fit, minimize
import scipy

from itertools import product
from datetime import datetime

import warnings
warnings.filterwarnings('ignore')

# To solve convergence problems?
# https://stackoverflow.com/questions/56164659/scipy-curve-fit-do-not-converge-even-if-i-iteratively-change-initial-guess


class Fit:
    """
    Base class of the fitting module. It stores ONE fit with original data and fitting parameters.

    There are some available functions ['linear', 'expo', 'power', 'nechad'] but other functions can be
    passed as parameter to the fitting

    Different metrics can be used to minimize the error ['r2', 'rmse', 'rmsle', 'SSE']
    Original data and fitted parameters keeps stored in the class.
    """
    def __init__(self, func, x, y, metrics=None, optimize_metric=None):
        """
        Fits a model (function) to a set of data (x, y), minimizing the optimize metric.
        Stores the parameters and the errors as a dictionary.
        :param func: math model y = f(x). Signature should be func_name(x, *params) -> y
        It can also be a string from the list: ['linear', 'expo', 'power', 'nechad']
        :param x: independent variable
        :param y: targets or dependent variable
        :param metrics: The metrics to be calculated after fitting. If none, use BaseFit.available_metrics
        :param optimize_metric: If informed, the algo will use the optimize metric's loss function to minimize
                                its errors. That's an extra step.
        """

        self.metrics = BaseFit.available_metrics if metrics is None else metrics
        self.func = Functions.fitting_func(func)

        # save initial parameters
        self.fit_params = {'func': self.func,
                           'qty': len(x),
                           'optimize_metric': optimize_metric,
                           'x': x,
                           'y': y,
                           }

        # initialize a log list to keep track of the logging
        self.log = []
        try:
            # make a non-linear least squares fit
            popt, pcov = curve_fit(self.func.func, x, y, check_finite=False, )

            # if there is a metric to be optimized, use the previous params to re-fit
            if optimize_metric is not None:
                res = minimize(optimize_metric.loss_func,  x0=popt,
                               args=(self.func.func, x, y), method='Nelder-Mead', tol=0.001)
                popt = res.x

            # update the fit_params with the results
            self.fit_params.update(BaseFit.test_fit(x, y, self.func, popt, metrics=self.metrics))

        except Exception as err:
            self.log.append(err)
            pass

    def html_metrics(self):
        """Return the fitted metrics formatted as HTML"""
        html_metrics = ''
        for metric in self.metrics:
            html_metrics += f'{metric}: {self.fit_params[metric]}<br>'
        return html_metrics

    def _scatter_plot(self, **kwargs):
        return px.scatter(x=self.fit_params['x'], y=self.fit_params['y'], **kwargs)

    def plot_fit(self, new_funcs=None, new_params=None, pts=100, fig=None, position=(1, 1), update_layout=None,
                 **kwargs):
        """
        Plots the fit (data and curve). If existing fig is passed, the position parameter indicates
        where (row, col tuple) it will be placed in a given figure.

        :param new_funcs: New function(s) to be added to the plot (tom compare the traces)
        :param new_params: The parameter(s) for the new function(s)
        :param pts: Number of points to be used in the function trace
        :param fig: Existing figure to collate this plot
        :param position: (row, column) of the fig to collate this new plot
        :param update_layout: At the end, update_layout function is called on the resulting figure to final adjustments
                              This parameter is a dictionary of update layout arguments ex. {'showlegend': False}
        :param kwargs: Other Plotly Express arguments that will be used when creating the Scatter of the points
        :return: Figure object
        """

        # create a scatter with the points
        scatter_plot = self._scatter_plot(**kwargs)

        fig = scatter_plot if fig is None else common.apply_subplot(fig, scatter_plot, position)

        # Create the original function trace if the function is fitted, otherwise, skip this trace
        if not self.is_empty:
            traces = [PlotFit.draw_func_trace(func=self.func,
                                              params=self.fit_params['params'],
                                              x_interval=self.fit_params['x'],
                                              txt=self.html_metrics())]
        else:
            traces = []

        # Append the traces for the "passed" functions (for comparison purposes)
        new_funcs, new_params = common.listify(new_funcs, new_params)
        for new_func, new_param in zip(new_funcs, new_params):
            traces.append(PlotFit.draw_func_trace(new_func, new_param, x_interval=self.fit_params['x']))

        # Add all function traces to the figure
        for trace in traces:
            fig.add_trace(trace, row=position[0], col=position[1])

        # After the traces are all drawn, call update layout to fine-tune
        fig.update_layout(update_layout)

        return fig

    def summary(self, params=None):
        """Returns just a subset of the fit_params"""
        params = BaseFit.summary_params if params is None else common.listify(params)

        res = {}
        for param in params:
            res[param] = self.get_param(param)

        return res

    def get_param(self, param_name, default_value=None):
        return self.fit_params.get(param_name, default_value)

    def test_fit(self, x, y):
        """Test the fit for new x and y data"""
        test_params = {'func': self.func,
                       'qty': len(x),
                       'optimize+metric': self.fit_params['optimize_metric'],
                       'x': x,
                       'y': y}

        test_params.update(BaseFit.test_fit(x, y, self.func, self.fit_params['params'], metrics=self.metrics))

        return test_params

    @property
    def is_empty(self):
        return 'params' not in self.fit_params.keys()

    def __repr__(self):
        s = f"{repr(self.func)} - qty: {self.fit_params['qty']} | "

        if self.is_empty:
            return s + 'Empty fitting'
        else:
            for key in self.metrics:
                s += f"{key}: {self.get_param(key)} "
            return s


class DfFit(Fit):
    """
    Derives from the Fit class and adds the support to work with DataFrames.
    """
    def __init__(self, df, func, expr_x, expr_y, metrics=None, optimize_metric=None):
        """Fits a function using the dataframe and expressions for x and for Y."""
        x = BaseFit.parse_expr_df(df, expr_x)
        y = BaseFit.parse_expr_df(df, expr_y)

        super().__init__(func, x=x, y=y, metrics=metrics, optimize_metric=optimize_metric)

        self.df = df
        self.expr_x = expr_x
        self.expr_y = expr_y

        self.fit_params.update({'idx': df.index.to_list(), 'band': expr_x, 'variable': expr_y})

    def predict(self, df):
        """Predict the values given this new dataframe"""
        x = BaseFit.parse_expr_df(df, self.expr_x)
        y = BaseFit.parse_expr_df(df, self.expr_y)

        return BaseFit.test_fit(x, y, self.fit_params['func'], self.fit_params['params'])

    def test_fit(self, df):
        """Test the fit for this new dataset"""
        x = BaseFit.parse_expr_df(df, self.expr_x)
        y = BaseFit.parse_expr_df(df, self.expr_y)

        test_params = super().test_fit(x, y)
        test_params['band'] = self.expr_x
        return test_params

    # ################################  PLOTTING METHODS  #################################
    def _scatter_plot(self, test_df=None, **kwargs):
        """Overrides the original scatter_plot from Fit, to pass the dataframe"""

        df = self.df if test_df is None else test_df

        x = BaseFit.parse_expr_df(df, self.expr_x)
        y = BaseFit.parse_expr_df(df, self.expr_y)
        return px.scatter(df, x=x, y=y, **kwargs)

    def plot_fit(self, new_funcs=None, new_params=None, fig=None, position=(1, 1), update_layout=None,
                 **kwargs):

        df = kwargs['test_df'] if 'test_df' in kwargs else self.df

        if 'hover_data' in kwargs:
            kwargs['hover_data'] = [df.index] + common.listify(kwargs['hover_data'])
        else:
            kwargs['hover_data'] = [df.index]

        # Use the plot from upper class Fit
        fig = super().plot_fit(position=position, new_funcs=new_funcs, new_params=new_params, fig=fig,
                               title=self.title, update_layout=update_layout, **kwargs)

        fig.update_xaxes(title_text=self.x_label, row=position[0], col=position[1])
        fig.update_yaxes(title_text=self.y_label, row=position[0], col=position[1])

        return fig

    def plot_pred_vs_targ(self, fig=None, position=None, update_layout=None, **kwargs):

        # Add the index to the hover_data argument
        if 'hover_data' in kwargs:
            kwargs['hover_data'] = [self.df.index] + common.listify(kwargs['hover_data'])
        else:
            kwargs['hover_data'] = [self.df.index]

        max_value = max(self.df[self.expr_y].max(), max(self.fit_params['y_hat']))
        min_value = min(self.df[self.expr_y].min(), min(self.fit_params['y_hat']))

        plot = px.scatter(self.df, x=self.expr_y, y=self.fit_params['y_hat'], **kwargs)
        plot.add_trace(go.Scatter(x=[min_value, max_value], y=[min_value, max_value], mode='lines'))

        fig = plot if fig is None else common.apply_subplot(fig, plot, position)

        fig.update_layout(update_layout)
        return fig

    # ################################  OTHER METHODS  #################################
    @property
    def title(self): return f'{repr(self.func)} on {self.expr_x}'

    @property
    def x_label(self):
        return f'Band {self.expr_x} nm' if self.expr_x in self.df.columns else f'Expression {self.expr_x}'

    @property
    def y_label(self): return self.expr_y

    def __repr__(self):
        return f'DfFit: Band {self.expr_x} ' + super().__repr__()

    def __len__(self):
        return len(self.df)


class MultiFit:
    """
    Uses parallel processing to fit several functions and several independent variables (lst_expr_x) to the
    dependent variable (expr_y) and return a dataframe with the results. Each fit is stored in the .fits list
    """
    def __init__(self, df, lst_expr_x, funcs, expr_y='SPM', metrics=None, metric=BaseFit.rmsle, n_cpus=None, pool=None,
                 optimize_metric=False):
        """
        Performs several DfFits and save the results to get best results.
        :param df: Dataframe with data to be fitted
        :param lst_expr_x: columns/expressions to be tested
        :param funcs: list of functions to be tested
        :param expr_y: independent variable
        :param metric: the metric to optimized
        :param n_cpus: Number of CPUs to use in the processing. If None, no parallel processing
        :param pool: existing pool to process the results. If None, a new pool is created
        :param optimize_metric: If True, go thorough a second step minimizing the loss function of the metric
        """

        self.start = datetime.now()
        self.df = df

        # If there was not multiprocessing, the solution would be a couple of for's loops to call DfFit
        # self.fits = []
        # for expr_x in lst_expr_x:
        #     for func in funcs:
        #         self.fits.append(DfFit(self.df, func, expr_x, expr_y, metric=metric))

        # However, to keep the code cleaner multi and single core will use MAPPING functions
        # First we have to create the arguments by combining the funcs and expressions
        # Prepare data to be sent to pool into an array by doing a cross product on the lists

        # if optimize metric is True, send the metric to be optimized as a parameter to the fit engine
        optimize_metric = metric if optimize_metric else None

        # data = np.array(list(product([df], funcs, lst_expr_x, [expr_y], [metrics], [optimize_metric])))
        data = np.array(list(product(funcs, lst_expr_x)))

        # In this part, we will exclude the combinations of that expect 1 independent variable and receives 2
        dims = np.fromiter(map(lambda x: x.dim, data[:, 0]), 'uint8')
        tuples = np.fromiter(map(lambda x: isinstance(x, tuple), data[:, 1]), 'bool')

        to_exclude = (dims == 1) & tuples

        self.data = np.delete(data, np.where(to_exclude), axis=0)

        with PoolManager(n_cpus, pool) as pool_mgr:
            # self.fits = pool_mgr.map(DfFit, data[:, 0], data[:, 1], data[:, 2], data[:, 3], data[:, 4], data[:, 5])
            self.fits = pool_mgr.map(DfFit,
                                     [df]*len(self.data),
                                     self.data[:, 0],
                                     self.data[:, 1],
                                     [expr_y]*len(self.data),
                                     [metrics]*len(self.data),
                                     [optimize_metric]*len(self.data))

        # Save the parameters
        self.lst_expr_x = lst_expr_x
        self.lst_funcs = funcs
        self.expr_y = expr_y
        self.metric = metric

        # At the end, we will keep the fits sorted by metric
        self.sort()
        self.end = datetime.now()

    @staticmethod
    def fit_filtered(df, lst_expr_x, funcs, filter_column, filter_value, thresh=10, expr_y='SPM', metric=BaseFit.rmsle,
                     n_cpus=None, pool=None, metrics=None, optimize_metric=None):
        """Filters the dataframe by column value and gets the best fit"""

        sub_df = df[df[filter_column] == filter_value]

        if len(sub_df) >= thresh:
            return MultiFit(sub_df, lst_expr_x, funcs, expr_y, metric=metric, n_cpus=n_cpus, pool=pool,
                            metrics=metrics, optimize_metric=optimize_metric)
        else:
            return None

    def sort(self, metric=None, reverse=False):
        """
        Sort the fits to put them from the best to the worst according to the metric (unless reverse is True)
        :param metric: metric for sorting
        :param reverse: if False (default), will put best fits first
        :return: Updates internal list of fits
        """
        # Update the metric or leave as is
        self.metric = metric if metric is not None else self.metric

        # Call the base function for sorting fits
        self.fits = BaseFit.sort_fits(self.fits, self.metric, reverse)

    def summary(self, params=None, n=5):

        params = BaseFit.summary_params if params is None else params

        # create a dictionary with groups and respective params. If group is none, fill it with NaNs
        results = {fit.title: fit.summary(params=params) if fit is not None else {param: None for param in params}
                   for fit in self.fits[:n]}

        return pd.DataFrame(results).T

    @property
    def elapsed_time(self):
        return f'{(self.end-self.start).total_seconds():.2f} seconds'

    # ################################  PLOTTING METHODS  #################################
    def plot_best_fits(self, n=6, cols=2, base_height=300, **kwargs):
        """
        Plot the fits in the MultiFit instance. Considering that the MultiFit can store enormous quantity of
        fits, this function will be limited to the "best" n fits, considering the fits are ordered by the
        given metric. If one wants to order differently, it is possible to sort the MultiFit.fits using
        BaseFit.sort_fits function and passing the corresponding metric.
        :param n: number of fits to plot
        :param cols: number of columns to divide the canvas
        :param base_height: high of each row
        :param kwargs: additional arguments to be used in the base px.scatter call
        :return: plotly figure
        """
        # First, we will check the max n in the range to avoid throwing exceptions
        n = min(n, len(self.fits))
        fits = self.fits[:n]

        return PlotFit.plot_fits(fits, cols, base_height, **kwargs)

    def plot_fits(self, plots, cols=1, base_height=300, **kwargs):

        # assure plot is a list of tuples
        plots = common.listify(plots)

        # get the desired fits
        fits = [self.get_fit(*plot) for plot in plots]

        return PlotFit.plot_fits(fits, cols, base_height, **kwargs)

    def plot_best_fit(self, **kwargs):
        return self.best_fit.plot_fit(**kwargs)

    def plot_metric(self, metric=None, max_n=50, fig=None, position=None):

        # Create the metric figure
        metric = self.metric if metric is None else metric

        x = [fit.title for fit in self.fits[:max_n]]
        y = [fit.get_param(metric) for fit in self.fits[:max_n]]

        df = pd.DataFrame({'Function': x, str(metric): y})
        metric_fig = px.line(df, x='Function', y=str(metric))

        # If there was an existing figure, just apply the new figure
        fig = metric_fig if fig is None else common.apply_subplot(fig, metric_fig, position)

        return fig

    # ################################  MANAGE FITS METHODS  #################################
    def get_fit(self, func, band):
        for fit in self.fits:
            if (fit.fit_params['func'] == func) and (fit.fit_params['band'] == band):
                return fit

    @property
    def best_fit(self): return self.fits[0]

    def __repr__(self):
        s = f'Multifit class with {len(self.fits)} fits on variable ({self.expr_y}): df with {len(self.df)} rows, bands:{self.lst_expr_x} funcs{self.lst_funcs} '
        return s

    def __len__(self):
        return len(self.df)

    #     def plot_multifit(self, rows, cols, plots, positions, base_height=300, **kwargs):
    #
    #         titles = [self.get_fit(*plot).title for plot in plots]
    # #         forlist(map(lambda x: f'Function {x[0]} on band {x[1]}', plots))
    #
    #         fig = plotly.subplots.make_subplots(rows=rows, cols=cols, subplot_titles=titles)
    #
    #         for plot, position in zip(plots, positions):
    #             fit = self.get_fit(*plot)
    #             fig = fit.plot_fit(fig=fig, position=position, **kwargs)
    #
    #         fig.update_layout(height=base_height*rows)
    #
    #         return fig.update_coloraxes(colorscale='Plasma')

    # def get_best_fit(self, metric='rmsle', criteria='min'):
    #     "Retrieve the summary of the best fit among the fits within this object"
    #     fits = self.get_results_df()
    #
    #     scores = fits.loc[(fits.index.levels[0], metric), :]
    #     idx_name, band = MultiFit.get_best_score(scores, criteria=criteria)
    #
    #     res_df = fits.loc[idx_name[0][0], band[0]]
    #     return res_df.append(pd.Series({'band': band[0], 'func': idx_name[0][0], 'qty': len(self.df), 'ids': self.df['Id'].to_list()}))
    #
    # def get_best_fit_obj(self, metric='rmsle', criteria='min'):
    #     "Retrieve the actual best fit object. Uses get_best_fit to decide among the results."
    #     best_fit = self.get_best_fit(metric=metric, criteria=criteria)
    #
    #     return self.get_fit(best_fit['func'], best_fit['band'])
    #
    # @staticmethod
    # def get_best_score(df, criteria='max', priority_funcs=['linear', 'power', 'nechad', 'expo']):
    #     "Given a dataframe with scores, get the best combination (function and band), that meets the logic criteria (max or min)"
    #
    #     # Get the function for the criteria
    #     criteria = pd.DataFrame.max if criteria=='max' else pd.DataFrame.min
    #
    #     # Get the lists of indexes and columns of elements that meet the criteria
    #     idxs, cols = np.where(df == criteria(criteria(df, skipna=True)))
    #
    #     # If there is more than one result, we have to get the one in the higher function piority
    #     idxs_names = df.index[idxs]
    #
    #     # Get the loc of the best solution given the priority functions
    #     loc = 0
    #     for func in priority_funcs:
    #         try:
    #             loc = idxs_names.get_loc(func)
    #
    #             # If the function appears more than once, gets the first result
    #             # TODO: update to work with priority bands combinations
    #             if isinstance(loc, np.ndarray): loc = np.where(loc == True)[0][0]
    #
    #             break
    #
    #         except:
    #             pass
    #
    #     idxs = [idxs[loc]] if isinstance(loc, np.int64) else idxs[loc]
    #     cols = [cols[loc]] if isinstance(loc, np.int64) else cols[loc]
    #     return df.index[idxs], df.columns[cols]
    #
    # #################################  OTHER METHODS  #################################
    # def get_title(self, metric='rmsle', criteria='min'):
    #     best_fit = self.get_best_fit_obj(metric=metric, criteria=criteria)
    #     return f'func {best_fit.func.__name__} on {best_fit.expr_x}'
    #
    # def get_results_df(self):
    #     results = {}
    #
    #     for expr_x in self.lst_expr_x:
    #         res_func = {}
    #         for func in self.lst_funcs:
    #             # get the saved fit
    #             fit = self.get_fit(func, expr_x)
    #
    #             # func must be in string format
    #             func = func if isinstance(func, str) else func.__name__
    #
    #             for key in self.results_vars:
    #                 res_func.update({(func, key): fit.fit_params[key] if fit and fit.filled() else None})
    #
    #         results.update({expr_x: res_func})
    #
    #     return pd.DataFrame.from_dict(results)
    #
    # def get_results_transposed(self, results_vars=None):
    #     results_vars = self.results_vars if results_vars is None else results_vars
    #
    #     results = {}
    #     for fit in self.fits:
    #         if fit.filled():
    #             values = {var: fit.fit_params[var] for var in results_vars}
    #             results.update({(fit.fit_params['func'], fit.fit_params['band']): values})
    #
    #     return pd.DataFrame(results).T


class GroupFit:
    def __init__(self, df, bands, funcs, group_column, expr_y='SPM', metric=BaseFit.rmsle, verbose=False,
                 ignore_none=True, thresh=10, n_cpus=None, pool=None, metrics=None, optimize_metric=False, ):
        """Loop through the unique values of the group_column and for each group,
        perform MultiFits according to bands and functions"""

        self.group_fits = {}

        with PoolManager(n_cpus, pool) as pool_mgr:
            for group in sorted(df[group_column].unique()):
                if verbose:
                    print(f'Fitting group={group}')
                group_fit = MultiFit.fit_filtered(df, bands, funcs, group_column, group, thresh=thresh,
                                                  expr_y=expr_y, metric=metric, n_cpus=n_cpus, pool=pool_mgr.pool,
                                                  metrics=metrics, optimize_metric=optimize_metric)

                if not ignore_none or (group_fit is not None):
                    self.group_fits[group] = group_fit
                else:
                    print(f'Problem fitting group {group}')

        self.df = df
        self.bands = bands
        self.funcs = funcs
        self.group_column = group_column
        self.variable = expr_y

        self.overall = self.calc_overall_metric()

    def summary(self, test_df=None, assign_bands=common.s2bands, params=None):

        # get the params to be displayed in the summary
        params = BaseFit.summary_params if params is None else params

        # if test_df is passed, will need to recalculate all the metrics
        if test_df is not None:
            return self.calc_test_metric(test_df, assign_bands, params)

        # create a dictionary with groups and respective params. If group is none, fill it with NaNs
        results = {group: fit.best_fit.summary(params=params) if fit is not None else {param: None for param in params}
                   for group, fit in self.group_fits.items()}

        # Add the overall with the desired params (the overall has already been created with all available metrics)
        results['overall'] = {param: self.overall[param] for param in params if param in self.overall}

        return pd.DataFrame.from_dict(results, orient='index')

    def sort_fits(self, metric=None, reverse=False):
        """
        Sort the fits of each group_fit (MultiFit instances) based on a metric
        :param metric: metric to be used for ordering the fits
        :param reverse: If True, each group will be sorted from the worst to the best
        :return: Each group_fit will be sorted internally and overall metric will be updated
        """
        for multi_fit in self.group_fits.values():
            multi_fit.sort(metric, reverse)

        self.overall = self.calc_overall_metric()

    def sort_groups(self, reverse=False):
        """
        Sort the groups (keys) in the .group_fit dictionary.
        :return: None
        """
        self.group_fits = {group: self[group] for group in sorted(self.group_fits.keys(), reverse=reverse)}

    def rename_groups(self, mapping):
        """
        Rename the groups according to a dictionary mapping {from_value: to_value}
        :return: The groups will be renamed accordingly
        """
        self.group_fits = {mapping[group]: self[group] for group in mapping if group in self.group_fits.keys()}

    def save_memory(self, max_n=10):
        """
        Save memory by deleting non-relevant fits and leaving just the best fits.
        :param max_n: maximum number of fits to be stored
        :return: None
        """
        for mfit in self.group_fits.values():
            mfit.fits = mfit.fits[:max_n]

    def assign_membership(self, df, bands):
        """
        Given a new dataframe - df, fill up the group column with corresponding clusters
        :param df: new dataframe to be assigned membership
        :param bands: bands used for testing membership
        :return: df will be added with a group column.
        """

        # create the covariance matrix of the bands for each group
        cov = self.df.groupby(by=self.group_column)[bands].cov()

        # get the mean reflectance for each group
        mean = self.df.groupby(by=self.group_column)[bands].mean()

        # calculate the distances for each row of df to the clusters
        # first we will create an empty dataframe
        distances_df = pd.DataFrame(index=df.index)

        for group in mean.index:
            # calc the distances to this group, just if the group has a fitting object
            if group not in self.group_fits:
                continue

            # First invert the covariance matrix for the group
            inv_cov = np.linalg.inv(cov.loc[group])

            # Calc Mahalanobis distances
            distances = df.apply(lambda row: scipy.spatial.distance.mahalanobis(row[bands],
                                                                                mean.loc[group],
                                                                                inv_cov),
                                 axis=1)

            distances_df[group] = distances

        # get the group, by using argmin
        args = distances_df.to_numpy().argmin(axis=1)
        clusters = list(map(lambda x: distances_df.columns[x], args))
        df[f'assigned_{self.group_column}'] = clusters

    def calc_test_metric(self, test_df, assign_bands=common.s2bands, params=None):

        # get the params to be displayed in the summary
        params = BaseFit.summary_params if params is None else params

        # if the grou column not in test_df, assign it
        self.assign_membership(test_df, bands=assign_bands)

        # create the name for the predictions column
        pred_column = self.variable + '_pred'

        # create a variable to store the results
        result = {}

        # first, we have to loop through the groups
        for group, multi_fit in self.group_fits.items():

            # assure that the fit for the group exists
            if multi_fit is None:
                continue

            # then, we check in the test_df if there are records in this group
            df = test_df[test_df[f'assigned_{self.group_column}'] == group]

            # if there is at least one item in the resulting dataframe
            if len(df) > 0:
                # calculate the prediction for the variable
                preds = multi_fit.best_fit.predict(df)
                test_df.loc[df.index, pred_column] = preds['y_hat']

                result[group] = multi_fit.best_fit.test_fit(df)

        result['overall'] = BaseFit.test_fit(test_df[pred_column], test_df['SPM'], func=None, metrics=BaseFit.available_metrics)
        result['overall'].pop('y_hat', None)

        result_df = pd.DataFrame(result).T
        return result_df[params]

    def calc_overall_metric(self):
        # prepare targets and predictions lists. These values are already saved in each best_fit
        preds, targs, groups = [], [], []

        # loop through the group_fits to get the predictions
        pred_column = self.variable + '_pred'
        for group, multi_fit in self.group_fits.items():
            if multi_fit is not None:
                self.df.loc[multi_fit.best_fit.get_param('idx'), pred_column] = multi_fit.best_fit.get_param('y_hat')

        # take care of nans
        df = self.df.dropna(subset=[pred_column, self.variable])
        if len(df) != len(self.df):
            print(f'Warning: {len(self.df)-len(df)} NaNs were found in {repr(self)}')

        # once we have all these values, let's calculate the metrics
        results = BaseFit.test_fit(df[pred_column], df[self.variable], func=None,
                                   metrics=BaseFit.available_metrics)
        results['qty'] = len(preds)
        results.pop('y_hat', None)
        return results

    def group_title(self, group): return 'Group ' + str(group) + ' - ' + self[group].best_fit.title

    # ################################  PLOT METHODS  #################################
    def plot_group(self, group, **kwargs):
        return self.group_fits[group].plot_best_fit(**kwargs)

    def plot_groups(self, groups=None, cols=2, base_height=300, **kwargs):

        # get the groups to be plotted
        groups = self.group_fits if groups is None else common.listify(groups)

        # create a base configuration and override it with the kwargs
        base_options = {'update_layout': {'showlegend': False}}
        base_options.update(kwargs)

        # get the fits to be plotted
        fits = [self.group_fits[group].best_fit for group in groups]

        # get the titles
        titles = [self.group_title(group) for group in self.group_fits]

        return PlotFit.plot_fits(fits, cols, base_height, titles=titles, **base_options)

    def plot_groups_summary(self, groups=None, max_n=50, base_height=500, **kwargs):
        # get the groups to be plotted
        groups = self.group_fits if groups is None else common.listify(groups)

        # create a base configuration and override it with the kwargs
        base_options = {'update_layout': {'showlegend': False}}
        base_options.update(kwargs)

        # get the titles
        titles = [[self.group_title(group), 'Actual vs Estimated'] for group in groups]
        titles = ['Actual vs Estimated (all vallues)'] + list(np.array(titles).reshape(-1))

        # create the subplots
        specs = [[{"colspan": 2}, None]] + [[{}, {}]] * len(groups)
        fig = plotly.subplots.make_subplots(rows=len(groups)+1, cols=2, specs=specs, subplot_titles=titles)

        common.apply_subplot(fig, self.plot_pred_vs_targ(log_x=True, log_y=True), (1, 1))

        # loop through the fits to plot them in the main figure
        for row, group in enumerate(groups):
            # Plot the curve fit
            self[group].best_fit.plot_fit(fig=fig, position=(row+2, 1), **kwargs)

            # Plot the metric decay
            # self[group].plot_metric(max_n=max_n, fig=fig, position=(row+2, 1))
            self[group].best_fit.plot_pred_vs_targ(fig=fig, position=(row+2, 2), log_x=True, log_y=True, **kwargs)

        # update the final layout
        update_layout = {'height': base_height * len(groups),
                         'showlegend': False}

        # Override with 'new' options from kwargs
        if 'update_layout' in kwargs:
            update_layout.update(kwargs['update_layout'])

        fig.update_layout(update_layout)

        fig.update(layout_coloraxis_showscale=False)
        return fig

    def plot_pred_vs_targ(self, test_df=None, color=None, title='', **kwargs):
        pred_column = self.variable + '_pred'
        # check if the variables are already in overall, update the metrics

        # get the correct dataframe to plot
        df = self.df if test_df is None else test_df

        # calculate the overall metric (again)
        overall = self.calc_test_metric(df).loc['overall']

        color = self.variable if color is None else color

        # force a color mapping so the cluster 0 has the first color and so on...
        colors = px.colors.qualitative.Plotly + px.colors.qualitative.Light24
        color_discrete_map = {key: colors[i] for i, key in enumerate(self.group_fits.keys())}

        fig = px.scatter(df, x=self.variable, y=pred_column, color=color, color_discrete_map=color_discrete_map,
                         hover_data=[df.index, 'Area', 'Station'], **kwargs)

        # trace the y=x line
        max_value = df[[self.variable, pred_column]].max().max()
        min_value = df[[self.variable, pred_column]].min().min()

        fig.add_trace(go.Scatter(x=[min_value, max_value], y=[min_value, max_value], mode='lines'))

        title = title + f"<br>R^2={overall['R^2']} | RMSLE={overall['RMSLE']} | RMSE={overall['RMSE']}"
        return fig.update_layout(title=title, showlegend=False)

    def __repr__(self):
        s = f'Grouped fitting with {len(self.group_fits)} groups: {list(self.group_fits.keys())}'
        return s

    def __getitem__(self, item):
        return self.group_fits[item]

